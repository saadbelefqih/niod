{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm8Y9d1YbDTQSzqvP7wSHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadbelefqih/niod/blob/main/Neural_Interpretable_Outlier_Detection_(NIOD).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3Sg8xJFsQd0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,Input,Lambda\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import plot_tree\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import backend as K\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, average_precision_score\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score\n",
        "from typing import Union, Tuple\n",
        "import torch.nn.functional as F\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JDrmKTXhsaZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"/content/drive/MyDrive/OutlierDetectionDiabetes/diabetes.csv\"\n",
        "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "# Load your dataset (replace 'your_data.csv' with your actual file)\n",
        "data = pd.read_csv(url)\n",
        "# Handle missing values (replace 'method' with your preferred imputation method)\n",
        "data = data.fillna(data.mean())\n",
        "# Normalize features\n",
        "#scaler = StandardScaler()\n",
        "#data[['glucose', 'blood_pressure', 'skin_thickness', 'insulin', 'bmi', 'age']] = scaler.fit_transform(data[['glucose', 'blood_pressure', 'skin_thickness', 'insulin', 'bmi', 'age']])\n",
        "# Split the data into features and target variable\n",
        "X = data.drop('Outcome', axis=1)\n",
        "#y = data['outcome']  # For evaluation purposes only\n",
        "print(\"X:\")\n",
        "print(X.head())"
      ],
      "metadata": {
        "id": "s52a1sSisfWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Concatenate\n",
        "import tensorflow_probability as tfp\n",
        "from sklearn.metrics import roc_curve, auc, classification_report,accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "class CentroidDistanceLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_shape, **kwargs):\n",
        "        super(CentroidDistanceLayer, self).__init__(**kwargs)\n",
        "        # Initialize centroid with a larger initial value\n",
        "        self.input_shape = input_shape\n",
        "        self.initial_centroid = [0.21565644, 0.15952514, -0.16965939, 0.24391688]\n",
        "        # Initialize centroid as a trainable variable with a larger initial value\n",
        "        self.centroid = self.add_weight(\n",
        "            shape=(input_shape,), # Shape based on input features\n",
        "            initializer=tf.keras.initializers.Constant(self.initial_centroid),  # Your initial value\n",
        "            trainable=True,  # Make it trainable\n",
        "            name='centroid'\n",
        "        )\n",
        "\n",
        "        # Store initial values as non-trainable weights for reference\n",
        "        self.initial_centroid_const = self.add_weight(\n",
        "            shape=(input_shape,),\n",
        "            initializer=tf.keras.initializers.Constant(self.initial_centroid),\n",
        "            trainable=False,\n",
        "            name='initial_centroid'\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        if self.centroid is None:\n",
        "            raise ValueError(\"Centroid has not been initialized. Call set_centroid() first.\")\n",
        "        # Ensure inputs and centroid have the same dtype\n",
        "        inputs = tf.cast(inputs, dtype=self.centroid.dtype)\n",
        "        # Calculate distance without expanding dimensions\n",
        "        distance = tf.reduce_sum(tf.square(inputs - self.centroid), axis=-1)\n",
        "        return tf.expand_dims(distance, -1)  # Expand dims for binary classification\n",
        "\n",
        "\n",
        "    \"\"\"def set_centroid(self, new_centroid):\n",
        "        self.centroid = tf.convert_to_tensor(new_centroid, dtype=tf.float32)\"\"\"\n",
        "\n",
        "    def get_centroid(self):\n",
        "        return self.centroid.numpy() if self.centroid is not None else None\n",
        "\n",
        "\n",
        "class BinaryClassificationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, threshold=0.11299490928649902, **kwargs):\n",
        "        super(BinaryClassificationLayer, self).__init__(**kwargs)\n",
        "        self.threshold = self.add_weight(\n",
        "            name='classification_threshold',\n",
        "            shape=(),\n",
        "            initializer=tf.keras.initializers.Constant(threshold),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Get probability scores from input\n",
        "        distances = inputs\n",
        "\n",
        "        # Apply threshold for binary classification\n",
        "        binary_output = tf.cast(distances >= self.threshold, tf.float32)\n",
        "        return binary_output\n",
        "\n",
        "    def set_threshold(self, new_threshold):\n",
        "        self.threshold.assign(new_threshold)\n",
        "\n",
        "    def get_threshold(self):\n",
        "        return self.threshold.numpy()\n",
        "\n",
        "\n",
        "class HybridNeuralMultiTreeModel:\n",
        "    def __init__(self, input_shape, n_components=10, embedding_dim=8, n_centroids=2,\n",
        "                 tree_max_depth=4, n_trees=2, distance_percentile=75, training=False, seed=42):\n",
        "        self.input_shape = input_shape\n",
        "        self.n_components = n_components\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_centroids = n_centroids\n",
        "        self.tree_max_depth = tree_max_depth\n",
        "        self.n_trees = n_trees\n",
        "        self.distance_percentile = distance_percentile\n",
        "        self.scaler = StandardScaler()\n",
        "        self.seed = seed\n",
        "        self.training = training\n",
        "        # Set random seed for reproducibility\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "        # If not training, set dropout rate to 0 to make it deterministic\n",
        "        self.dropout_rate = 0.2 if training else 0.0\n",
        "        # Build the main model\n",
        "        self.model, self.dim_reduction_model, self.embedding_model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        inputs = Input(shape=self.input_shape, name=\"input_layer\")\n",
        "\n",
        "        # Dimensionality reduction layers\n",
        "        x = Dense(self.n_components, activation='relu', name='dim_reduction_1',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=self.seed))(inputs)\n",
        "        x = Dropout(self.dropout_rate)(x, training=self.training)\n",
        "        x = Dense(self.n_components // 2, activation='relu', name='dim_reduction_2',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=self.seed))(x)\n",
        "\n",
        "        # Embedding layers\n",
        "        e = Dense(self.embedding_dim, activation='tanh',name='embedding_1',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=self.seed))(x)\n",
        "        e = Dropout(self.dropout_rate)(e, training=self.training)\n",
        "        e = Dense(self.embedding_dim // 2, activation='tanh',name='embedding_2',kernel_initializer=tf.keras.initializers.GlorotUniform(seed=self.seed))(e)\n",
        "\n",
        "        # CentroidDistance layers\n",
        "        self.centroid_layer = CentroidDistanceLayer(input_shape=e.shape[-1])\n",
        "        outlier_score = self.centroid_layer(e)\n",
        "\n",
        "        # Binary Classification Layer\n",
        "        self.classificationLayer = BinaryClassificationLayer()\n",
        "        output = self.classificationLayer(outlier_score)  # Get the output tensor\n",
        "\n",
        "\n",
        "        # Create models\n",
        "        main_model = Model(inputs=inputs, outputs=output, name=\"Hybrid_Neural_Tree\")\n",
        "\n",
        "        @tf.function\n",
        "        def custom_loss(y_true, y_pred):\n",
        "            # Label smoothing\n",
        "            label_smoothing = 0.1\n",
        "            y_true = y_true * (1 - label_smoothing) + 0.5 * label_smoothing\n",
        "\n",
        "            # Binary cross-entropy\n",
        "            bce = tf.keras.losses.binary_crossentropy(\n",
        "                y_true,\n",
        "                y_pred,\n",
        "                from_logits=False\n",
        "            )\n",
        "\n",
        "            # Weights for different loss components\n",
        "            centroid_preservation_weight = 5.0  # Increased weight for centroid preservation\n",
        "            class_balance_weight = 0.5\n",
        "            centroid_regularization_weight = 0.1\n",
        "\n",
        "            # Centroid preservation loss (penalize deviation from initial values)\n",
        "            centroid_preservation_loss = tf.reduce_mean(\n",
        "                tf.square(self.centroid_layer.centroid - self.centroid_layer.initial_centroid_const)\n",
        "            )\n",
        "\n",
        "            # Original regularization terms\n",
        "            centroid_mean = tf.reduce_mean(self.centroid_layer.centroid)\n",
        "            centroid_regularization = tf.reduce_sum(tf.square(self.centroid_layer.centroid - centroid_mean))\n",
        "            class_balance = class_balance_weight * tf.square(tf.reduce_mean(y_pred) - 0.5)\n",
        "\n",
        "            # Combined loss with strong weight on centroid preservation\n",
        "            total_loss = (bce +\n",
        "                         centroid_preservation_weight * centroid_preservation_loss +\n",
        "                         centroid_regularization_weight * centroid_regularization +\n",
        "                         class_balance)\n",
        "\n",
        "            return total_loss\n",
        "\n",
        "        main_model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "            loss=custom_loss,\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        dim_reduction_model = Model(inputs=inputs, outputs=x, name=\"Dimensionality_Reduction_Model\")\n",
        "        embedding_model = Model(inputs=x, outputs=e, name=\"Embedding_Model\")\n",
        "\n",
        "        return main_model, dim_reduction_model, embedding_model\n",
        "\n",
        "    def fit(self, X, epochs=200, batch_size=32, validation_split=0.2):\n",
        "        #scaler = StandardScaler()\n",
        "        #X_scaled = scaler.fit_transform(X)\n",
        "        X_scaled = X\n",
        "\n",
        "        # Get initial embeddings\n",
        "        reduced_features = self.dim_reduction_model.predict(X_scaled)\n",
        "        embedding_features = self.embedding_model.predict(reduced_features)\n",
        "\n",
        "        centroid = np.mean(embedding_features, axis=0)\n",
        "        # Calculate distances of each point from the centroid in latent space\n",
        "        #distances = cdist(embedding_features, centroid.reshape(1, -1), metric='euclidean').flatten()\n",
        "        distances = tf.reduce_sum(tf.square(embedding_features - centroid), axis=-1)\n",
        "        distances=tf.expand_dims(distances, -1)\n",
        "        # Set a threshold for outlier detection based on the 95th percentile of distances\n",
        "        threshold = np.percentile(distances, 75)\n",
        "        # Initialize centroids with normalized data\n",
        "        tf.print(\"fit centroid:\", centroid)\n",
        "        tf.print(\"fit threshold:\", threshold)\n",
        "\n",
        "        #self.centroid_layer.set_weights([centroid])\n",
        "        #self.classificationLayer.set_weights([np.array(threshold)])\n",
        "        # Normalize centroids\n",
        "        #initial_centroids = initial_centroids / np.linalg.norm(initial_centroids, axis=1, keepdims=True)\n",
        "        #self.centroid_layer.set_centroid(centroid)\n",
        "\n",
        "        # Calculate adaptive threshold\n",
        "        #self.classificationLayer.set_threshold(threshold)\n",
        "\n",
        "        # Generate balanced binary labels\n",
        "        #binary_labels = (distances > threshold).astype(int)\n",
        "        #binary_labels = binary_labels.reshape(-1, 1)\n",
        "        binary_labels = tf.cast(distances >= threshold, tf.float32).numpy()\n",
        "\n",
        "        # Training callbacks\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=15,  # Increased patience\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,  # More aggressive LR reduction\n",
        "                patience=10,\n",
        "                min_lr=0.00001\n",
        "            ),\n",
        "            # Add callback to monitor centroid values\n",
        "            tf.keras.callbacks.LambdaCallback(\n",
        "                on_epoch_end=lambda epoch, logs: tf.print(\n",
        "                    \"\\nCurrent centroids:\", self.centroid_layer.centroid,\n",
        "                    \"\\nTarget centroids:\", self.centroid_layer.initial_centroid_const,\n",
        "                    \"\\nDifference:\", tf.reduce_mean(tf.abs(\n",
        "                        self.centroid_layer.centroid - self.centroid_layer.initial_centroid_const\n",
        "                    ))\n",
        "                ) if epoch % 10 == 0 else None\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        history = self.model.fit(\n",
        "            X_scaled, binary_labels,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Train tree ensemble\n",
        "        self.tree_classifier = BaggingClassifier(\n",
        "            estimator=DecisionTreeClassifier(max_depth=self.tree_max_depth),\n",
        "            n_estimators=self.n_trees,\n",
        "            random_state=42\n",
        "        )\n",
        "        self.tree_classifier.fit(X_scaled, binary_labels.ravel())\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def get_threshold(self):\n",
        "        return self.classificationLayer.get_threshold()\n",
        "\n",
        "    def get_centroid(self):\n",
        "        return self.centroid_layer.get_centroid()\n",
        "\n",
        "    def evaluate_performance(self, X_test, y_true):\n",
        "        \"\"\"\n",
        "        Evaluates the model's performance on test data and plots accuracy, precision, recall, F1-score, and confusion matrix.\n",
        "\n",
        "        Parameters:\n",
        "        X_test (ndarray): Test feature data.\n",
        "        y_true (ndarray): True binary labels for the test data.\n",
        "        \"\"\"\n",
        "        # Obtain binary predictions from centroid-based model\n",
        "        y_true = y_true.astype(int).ravel()\n",
        "        prediction = self.predict(X_test)\n",
        "        print(\"evaluate_performance = prediction\")\n",
        "        #print(prediction)\n",
        "        print(\"************************\")\n",
        "        # Calculate overall  metrics\n",
        "        acc = accuracy_score(y_true, prediction)\n",
        "        prec = precision_score(y_true, prediction)\n",
        "        rec= recall_score(y_true, prediction)\n",
        "        f1 = f1_score(y_true, prediction)\n",
        "        # Calculate per-class metrics using classification_report\n",
        "        report = classification_report(y_true, prediction, target_names=['Normal', 'Outlier'], output_dict=True)\n",
        "        # 1. Metrics Bar Plot\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "        values = [acc, prec, rec, f1]\n",
        "        colors = ['#2ecc71', '#3498db', '#e74c3c', '#f1c40f']\n",
        "\n",
        "        plt.bar(metrics, values, color=colors)\n",
        "        plt.ylim(0, 1)\n",
        "        plt.title('Model Performance Metrics', pad=20)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar in plt.gca().patches:\n",
        "            height = bar.get_height()\n",
        "            plt.gca().text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.3f}',\n",
        "                    ha='center', va='bottom')\n",
        "        plt.show()\n",
        "\n",
        "        # 2. Per-Class Metrics Plot\n",
        "        # Create a pandas DataFrame for better visualization\n",
        "        df_report = pd.DataFrame(report).transpose()\n",
        "\n",
        "        print(\"Per-Class Metrics:\")\n",
        "        print(df_report)\n",
        "        # 2. Confusion Matrix\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "        cm = confusion_matrix(y_true, prediction)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.show()\n",
        "\n",
        "        # 3. ROC Curve\n",
        "        fig3,ax3 = plt.subplots(figsize=(6, 6))\n",
        "        # Get the distance scores (before thresholding)\n",
        "        reduced_features = self.dim_reduction_model.predict(X_test)\n",
        "        embedding_features = self.embedding_model.predict(reduced_features)\n",
        "        distances = self.centroid_layer(embedding_features).numpy()\n",
        "\n",
        "        # Calculate ROC curve and AUC\n",
        "        fpr, tpr, _ = roc_curve(y_true, distances)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plot ROC curve\n",
        "        ax3.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "        ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        ax3.set_xlim([0.0, 1.0])\n",
        "        ax3.set_ylim([0.0, 1.05])\n",
        "        ax3.set_xlabel('False Positive Rate')\n",
        "        ax3.set_ylabel('True Positive Rate')\n",
        "        ax3.set_title('Receiver Operating Characteristic')\n",
        "        ax3.legend(loc=\"lower right\")\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.show()\n",
        "        # 4. Class Distribution\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        class_dist = np.bincount(y_true.astype(int)) / len(y_true) * 100\n",
        "        plt.pie(class_dist, labels=['Normal', 'Outlier'], autopct='%1.1f%%',\n",
        "                colors=['#3498db', '#e74c3c'])\n",
        "        plt.title('Class Distribution')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 2. Feature Importance Analysis\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Get feature importance from tree ensemble\n",
        "        importances = np.mean([tree.feature_importances_ for tree in self.tree_classifier.estimators_], axis=0)\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'feature': [f'Feature {i}' for i in range(X_test.shape[1])],\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=True)\n",
        "\n",
        "        plt.barh(range(len(feature_importance_df)), feature_importance_df['importance'])\n",
        "        plt.yticks(range(len(feature_importance_df)), feature_importance_df['feature'])\n",
        "        plt.xlabel('Feature Importance')\n",
        "        plt.title('Feature Importance Analysis')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 3. Distance Distribution Analysis\n",
        "        reduced_features = self.dim_reduction_model.predict(X_test)\n",
        "        embedding_features = self.embedding_model.predict(reduced_features)\n",
        "        distances = self.centroid_layer(embedding_features).numpy()\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(distances[y_true == 0], bins=30, alpha=0.5, label='Normal', color='blue')\n",
        "        plt.hist(distances[y_true == 1], bins=30, alpha=0.5, label='Outlier', color='red')\n",
        "        plt.axvline(self.get_threshold(), color='green', linestyle='--', label='Decision Threshold')\n",
        "        plt.xlabel('Distance from Centroid')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Distance Distribution Analysis')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        # 4. Embedding Space Visualization (if embedding_dim <= 2)\n",
        "        if embedding_features.shape[1] <= 2:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.scatter(embedding_features[y_true == 0, 0],\n",
        "                      embedding_features[y_true == 0, 1] if embedding_features.shape[1] > 1 else np.zeros_like(embedding_features[y_true == 0]),\n",
        "                      c='blue', label='Normal', alpha=0.6)\n",
        "            plt.scatter(embedding_features[y_true == 1, 0],\n",
        "                      embedding_features[y_true == 1, 1] if embedding_features.shape[1] > 1 else np.zeros_like(embedding_features[y_true == 1]),\n",
        "                      c='red', label='Outlier', alpha=0.6)\n",
        "            centroid = self.get_centroid()\n",
        "            plt.scatter(centroid[0], centroid[1] if len(centroid) > 1 else 0,\n",
        "                      c='green', marker='*', s=200, label='Centroid')\n",
        "            plt.title('Embedding Space Visualization')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "        # 5. Local Interpretation Analysis\n",
        "        def get_local_importance(instance, n_samples=1000):\n",
        "            # Create perturbed samples around the instance\n",
        "            perturbed = np.tile(instance, (n_samples, 1))\n",
        "            noise = np.random.normal(0, 0.1, perturbed.shape)\n",
        "            perturbed = perturbed + noise\n",
        "\n",
        "            # Get predictions for perturbed samples\n",
        "            predictions = self.predict(perturbed)\n",
        "\n",
        "            # Calculate feature importance based on correlation with predictions\n",
        "            local_importance = np.zeros(instance.shape[0])\n",
        "            for i in range(instance.shape[0]):\n",
        "                local_importance[i] = np.abs(np.corrcoef(perturbed[:, i], predictions.ravel())[0, 1])\n",
        "\n",
        "            return local_importance\n",
        "\n",
        "        # Select a few instances for local interpretation\n",
        "        n_examples = 3\n",
        "        example_indices = np.random.choice(len(X_test), n_examples)\n",
        "\n",
        "        plt.figure(figsize=(15, 5 * n_examples))\n",
        "        for i, idx in enumerate(example_indices):\n",
        "            local_importance = get_local_importance(X_test[idx])\n",
        "\n",
        "            plt.subplot(n_examples, 1, i + 1)\n",
        "            plt.barh(range(len(local_importance)), local_importance)\n",
        "            plt.yticks(range(len(local_importance)), [f'Feature {j}' for j in range(len(local_importance))])\n",
        "            plt.xlabel('Local Feature Importance')\n",
        "            plt.title(f'Local Interpretation for Instance {idx} (True Label: {y_true[idx]}, Predicted: {prediction[idx]})')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print additional interpretability metrics\n",
        "        print(\"\\nInterpretability Metrics:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Calculate decision stability\n",
        "        noise = np.random.normal(0, 0.1, X_test.shape)\n",
        "        noisy_predictions = self.predict(X_test + noise)\n",
        "        stability = np.mean(prediction == noisy_predictions)\n",
        "        print(f\"Decision Stability: {stability:.3f}\")\n",
        "\n",
        "        # Calculate average distance to decision boundary\n",
        "        threshold = self.get_threshold()\n",
        "        avg_distance_to_boundary = np.mean(np.abs(distances - threshold))\n",
        "        print(f\"Average Distance to Decision Boundary: {avg_distance_to_boundary:.3f}\")\n",
        "\n",
        "        # Feature importance summary\n",
        "        print(\"\\nTop 5 Most Important Features:\")\n",
        "        top_features = feature_importance_df.iloc[-5:].iloc[::-1]\n",
        "        for idx, row in top_features.iterrows():\n",
        "            print(f\"{row['feature']}: {row['importance']:.3f}\")\n",
        "\n",
        "\n",
        "    def visualize_decision_trees(model, feature_names=None, class_names=['Normal', 'Outlier'], max_trees=4):\n",
        "        \"\"\"\n",
        "        Visualize the decision trees from the BaggingClassifier ensemble\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model : HybridNeuralMultiTreeModel\n",
        "            The trained hybrid model containing the tree ensemble\n",
        "        feature_names : list, optional\n",
        "            List of feature names for the input data\n",
        "        class_names : list, optional\n",
        "            Names for the classes (default: ['Normal', 'Outlier'])\n",
        "        max_trees : int, optional\n",
        "            Maximum number of trees to display (default: 4)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get the trees from the bagging classifier\n",
        "        trees = model.tree_classifier.estimators_\n",
        "        n_trees = min(len(trees), max_trees)\n",
        "\n",
        "        # Calculate the grid layout\n",
        "        n_cols = min(2, n_trees)\n",
        "        n_rows = math.ceil(n_trees / n_cols)\n",
        "\n",
        "        # Create figure with subplots\n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15 * n_cols, 10 * n_rows))\n",
        "        if n_trees == 1:\n",
        "            axes = np.array([axes])\n",
        "        axes = axes.ravel()  # Flatten axes array for easier indexing\n",
        "\n",
        "        # If feature names not provided, create generic ones\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'Feature {i}' for i in range(model.input_shape)]\n",
        "\n",
        "        # Plot each tree\n",
        "        for idx, tree in enumerate(trees[:max_trees]):\n",
        "            # Plot the decision tree\n",
        "            plot_tree(\n",
        "                tree,\n",
        "                feature_names=feature_names,\n",
        "                class_names=class_names,\n",
        "                filled=True,\n",
        "                rounded=True,\n",
        "                ax=axes[idx],\n",
        "                proportion=True,\n",
        "                precision=2\n",
        "            )\n",
        "            axes[idx].set_title(f'Decision Tree {idx + 1}')\n",
        "\n",
        "        # Remove empty subplots if any\n",
        "        for idx in range(n_trees, len(axes)):\n",
        "            fig.delaxes(axes[idx])\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Add information about the model\n",
        "        info_text = (\n",
        "            f\"Model Information:\\n\"\n",
        "            f\"Number of trees in ensemble: {len(trees)}\\n\"\n",
        "            f\"Max tree depth: {model.tree_max_depth}\\n\"\n",
        "            f\"Input shape: {model.input_shape}\\n\"\n",
        "            f\"Embedding dimension: {model.embedding_dim}\"\n",
        "        )\n",
        "\n",
        "        plt.figtext(0.02, 0.02, info_text, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def plot_feature_importance(model, feature_names=None):\n",
        "        \"\"\"\n",
        "        Plot feature importance across all trees in the ensemble\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model : HybridNeuralMultiTreeModel\n",
        "            The trained hybrid model containing the tree ensemble\n",
        "        feature_names : list, optional\n",
        "            List of feature names for the input data\n",
        "        \"\"\"\n",
        "\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'Feature {i}' for i in range(model.input_shape)]\n",
        "\n",
        "        # Calculate mean feature importance across all trees\n",
        "        importances = np.mean([\n",
        "            tree.feature_importances_\n",
        "            for tree in model.tree_classifier.estimators_\n",
        "        ], axis=0)\n",
        "\n",
        "        # Sort features by importance\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "\n",
        "        # Create figure\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.title('Feature Importance Across Decision Trees')\n",
        "        plt.bar(range(len(importances)), importances[indices])\n",
        "        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        return plt.gcf()"
      ],
      "metadata": {
        "id": "k5lqM4x2sldH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train, X_test, y_train, y_test\n",
        "# Initialize model\n",
        "model = HybridNeuralMultiTreeModel(\n",
        "    input_shape=(X_train.shape[1],),\n",
        "    n_centroids=2,\n",
        "    tree_max_depth=5,\n",
        "    n_trees=3\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train)\n",
        "\n",
        "# Get the threshold value\n",
        "threshold = model.get_threshold()\n",
        "print(f\"Outlier detection threshold: {threshold}\")\n",
        "\n",
        "centroids = model.get_centroid()\n",
        "print(f\"Outlier detection centroids: {centroids}\")"
      ],
      "metadata": {
        "id": "o7FYw71isnNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate_performance(X_test, y_test)"
      ],
      "metadata": {
        "id": "iJ0X52acso-l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}